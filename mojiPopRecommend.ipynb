{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectionFilePath = 'F:\\MojiPop\\collection.csv'\n",
    "usersFilePath = 'F:\\MojiPop\\\\users.csv'\n",
    "caricaturesFilePath = 'F:\\MojiPop\\caricatures.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(filePath):\n",
    "    csvData = []\n",
    "    with open(filePath) as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        csvHeader = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            csvData.append(row)\n",
    "    return csvData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['65702', '121000900710 ']\n"
     ]
    }
   ],
   "source": [
    "collectionRawData = readCsv(collectionFilePath)\n",
    "#print(len(collectionRawData))\n",
    "collectionData = []\n",
    "for row in collectionRawData:\n",
    "    tempData = row[0].split(';')\n",
    "    collectionData.append(tempData)\n",
    "print(collectionData[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4642, 19)\n"
     ]
    }
   ],
   "source": [
    "usersRawData = readCsv(usersFilePath)\n",
    "#print(len(usersRawData))\n",
    "usersData = []\n",
    "for row in usersRawData:\n",
    "    tempData = row[0].split(';')\n",
    "    tempData[2] = int(tempData[2])\n",
    "    usersData.append(tempData)\n",
    "#print(usersData[2])\n",
    "\n",
    "usersMatrix = []\n",
    "for row in usersData:\n",
    "    gender_F = 1 if(row[1]=='F') else 0\n",
    "    gender_M = 1 if(row[1]=='M') else 0\n",
    "    gender_UNK = 1 if(row[1]=='') else 0\n",
    "    age_UNK = 1 if(row[2]<=0) else 0\n",
    "    age_BABY = 1 if(row[2]>0 and row[2]<=3) else 0\n",
    "    age_TEEN = 1 if(row[2]>3 and row[2]<=14) else 0\n",
    "    age_YOUNG = 1 if(row[2]>14 and row[2]<=29) else 0\n",
    "    age_MIDDLE = 1 if(row[2]>29 and row[2]<=55) else 0\n",
    "    age_OLD = 1 if(row[2]>55) else 0\n",
    "    lang_EN = 1 if(row[3]=='en') else 0\n",
    "    lang_PT = 1 if(row[3]=='pt') else 0\n",
    "    lang_ZH_TW = 1 if(row[3]=='zh_tw') else 0\n",
    "    lang_FR = 1 if(row[3]=='fr') else 0\n",
    "    lang_KO = 1 if(row[3]=='ko') else 0\n",
    "    lang_RU = 1 if(row[3]=='ru') else 0\n",
    "    lang_ES = 1 if(row[3]=='es') else 0\n",
    "    lang_AR = 1 if(row[3]=='ar') else 0\n",
    "    lang_ZH = 1 if(row[3]=='zh') else 0\n",
    "    lang_JA = 1 if(row[3]=='ja') else 0\n",
    "    tempData = [gender_F, gender_M, gender_UNK, \n",
    "                age_UNK, age_BABY, age_TEEN, age_YOUNG, age_MIDDLE, age_OLD,\n",
    "                lang_EN, lang_PT, lang_ZH_TW, lang_FR, lang_KO, lang_RU, lang_ES, lang_AR, lang_ZH, lang_JA]\n",
    "    usersMatrix.append(tempData)\n",
    "usersMatrix = np.array(usersMatrix, dtype=float)\n",
    "#print(usersMatrix)\n",
    "print(usersMatrix.shape)\n",
    "\n",
    "usersIndex = []\n",
    "for i in range(len(usersData)):\n",
    "    usersIndex.append(usersData[i][0])\n",
    "#print(usersIndex[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n"
     ]
    }
   ],
   "source": [
    "caricaturesRawData = readCsv(caricaturesFilePath)\n",
    "#print(caricaturesRawData[0])\n",
    "\n",
    "caricaturesOneHot = tf.one_hot(np.linspace(0,len(caricaturesRawData)-1,len(caricaturesRawData)),len(caricaturesRawData))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    caricaturesMatrix = sess.run(caricaturesOneHot)\n",
    "print(np.size(caricaturesMatrix,0))\n",
    "\n",
    "caricaturesIndex = []\n",
    "for i in range(len(caricaturesRawData)):\n",
    "    caricaturesIndex.append(caricaturesRawData[i][0])\n",
    "#print(caricaturesIndex[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4642\n",
      "[ 1. 16.  1. ... 11. 10.  7.]\n"
     ]
    }
   ],
   "source": [
    "collectionMatrix = np.zeros([4642,1280])\n",
    "\n",
    "for collection in collectionData:\n",
    "    userNo = usersIndex.index(collection[0])\n",
    "    caricatureNo = caricaturesIndex.index(collection[1])\n",
    "    #print(caricaturesMatrix[caricatureNo].sum())\n",
    "    collectionMatrix[userNo] = collectionMatrix[userNo] + caricaturesMatrix[caricatureNo]\n",
    "print(collectionMatrix.shape[0])\n",
    "print(collectionMatrix.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.placeholder(tf.float32, [None, 19])\n",
    "x1 = tf.placeholder(tf.float32, [None, 1280])\n",
    "y = tf.placeholder(tf.float32, [None, 1280])\n",
    "dropout_rate = tf.placeholder(tf.float32)\n",
    "hyperP = 200\n",
    "generalization_rate = 1280*0.20\n",
    "\n",
    "\n",
    "v0 = tf.get_variable('v0', [x0.get_shape()[1], hyperP], \n",
    "                    initializer = tf.random_normal_initializer(0.00,0.01))\n",
    "v1 = tf.get_variable('v1', [x1.get_shape()[1], hyperP],\n",
    "                    initializer = tf.random_normal_initializer(0.00,0.01))\n",
    "embedding0 = tf.matmul(x0, v0)\n",
    "embedding1 = tf.matmul(x1, v1)\n",
    "embedding = tf.concat([embedding0, embedding1], 1)\n",
    "\n",
    "# FM part\n",
    "w0 = tf.get_variable('w0', [tf.concat([x0, x1],1).get_shape()[1], hyperP], \n",
    "                    initializer = tf.random_normal_initializer(0.00, 1.00))\n",
    "b0 = tf.get_variable('b0', [hyperP],\n",
    "                    initializer = tf.random_normal_initializer(0.00, 1.00))\n",
    "linear_term = tf.add(tf.matmul(tf.concat([x0,x1],1), w0), b0)\n",
    "\n",
    "interaction_term = tf.multiply(embedding0, embedding1)\n",
    "y_fm = tf.concat([linear_term, interaction_term], axis=1)\n",
    "\n",
    "\n",
    "#DNN part\n",
    "embedding = tf.nn.dropout(embedding, dropout_rate)\n",
    "glorot = tf.sqrt(2.0/(tf.cast(embedding.get_shape()[1], tf.float32) + hyperP))\n",
    "w1 = tf.get_variable('w1', [embedding.get_shape()[1], hyperP],\n",
    "                    initializer = tf.random_normal_initializer(0.00, glorot))\n",
    "b1 = tf.get_variable('b1', [hyperP], \n",
    "                    initializer = tf.random_normal_initializer(0.00, glorot))\n",
    "y_hidden1 = tf.nn.relu(tf.matmul(embedding, w1) + b1)\n",
    "y_hidden1 = tf.nn.dropout(y_hidden1, dropout_rate)\n",
    "glorot = tf.sqrt(1/hyperP)\n",
    "w2 = tf.get_variable('w2', [hyperP, hyperP],\n",
    "                    initializer = tf.random_normal_initializer(0.00, glorot))\n",
    "b2 = tf.get_variable('b2', [hyperP], \n",
    "                    initializer = tf.random_normal_initializer(0.00, glorot))\n",
    "y_hidden2 = tf.nn.relu(tf.matmul(y_hidden1, w2) + b2)\n",
    "y_hidden2 = tf.nn.dropout(y_hidden2, dropout_rate)\n",
    "w3 = tf.get_variable('w3', [hyperP, hyperP],\n",
    "                    initializer = tf.random_normal_initializer(0.00, glorot))\n",
    "b3 = tf.get_variable('b3', [hyperP], \n",
    "                    initializer = tf.random_normal_initializer(0.00, glorot))\n",
    "y_dnn = tf.nn.relu(tf.matmul(y_hidden2, w3) + b3)\n",
    "y_dnn = tf.nn.dropout(y_dnn, dropout_rate)\n",
    "\n",
    "#out\n",
    "glorot = tf.sqrt(2.0/(tf.cast(y_fm.get_shape()[1], tf.float32) + \n",
    "                      tf.cast(y_dnn.get_shape()[1], tf.float32) + 1))\n",
    "w_sparse = tf.get_variable('w_sparse', [y_fm.get_shape()[1]+y_dnn.get_shape()[1], y.get_shape()[1]],\n",
    "                    initializer = tf.random_normal_initializer(0.00, glorot))\n",
    "b_sparse = tf.get_variable('b_sparse', [y.get_shape()[1]], \n",
    "                    initializer = tf.constant_initializer(0.01))\n",
    "y_all = tf.concat([y_fm, y_dnn], 1)\n",
    "y_out = tf.add(tf.matmul(y_all, w_sparse), b_sparse)\n",
    "y_out_prob = tf.nn.sigmoid(y_out)\n",
    "\n",
    "#loss\n",
    "y_out_generalization = tf.multiply(y_out_prob, y)\n",
    "loss = generalization_rate * tf.reduce_mean(tf.square(y_out_generalization - y)) +  tf.reduce_mean(tf.square(y_out_prob - y))\n",
    "\n",
    "#accuracy\n",
    "predict = tf.cast(tf.greater(y_out_prob, 0.5), tf.float32)\n",
    "#correct_prediction = tf.equal(predict, y)\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "predict_generalization = tf.multiply(predict, y)\n",
    "accuracy = tf.count_nonzero(predict_generalization)/tf.count_nonzero(y)\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echos: 0\n",
      "[Train] loss=1.70822  accuracy:0.50155\n",
      "[Train] loss=1.27882  accuracy:0.63563\n",
      "[Train] loss=1.15930  accuracy:0.77215\n",
      "[Train] loss=0.83232  accuracy:0.84737\n",
      "[Train] loss=0.65179  accuracy:0.88623\n",
      "[Train] loss=0.58694  accuracy:0.89964\n",
      "[Train] loss=0.50388  accuracy:0.90006\n",
      "[Train] loss=0.49064  accuracy:0.91269\n",
      "[Test] loss=0.46923  accuracy:0.91907\n",
      "echos: 1\n",
      "[Train] loss=0.46189  accuracy:0.95196\n",
      "[Train] loss=0.43956  accuracy:0.95331\n",
      "[Train] loss=0.41225  accuracy:0.96563\n",
      "[Train] loss=0.37157  accuracy:0.96551\n",
      "[Train] loss=0.33037  accuracy:0.96457\n",
      "[Train] loss=0.32875  accuracy:0.95287\n",
      "[Train] loss=0.29561  accuracy:0.93741\n",
      "[Train] loss=0.28717  accuracy:0.94531\n",
      "[Test] loss=0.34867  accuracy:0.91135\n",
      "echos: 2\n",
      "[Train] loss=0.27332  accuracy:0.97371\n",
      "[Train] loss=0.27644  accuracy:0.96807\n",
      "[Train] loss=0.26147  accuracy:0.97788\n",
      "[Train] loss=0.24724  accuracy:0.97239\n",
      "[Train] loss=0.23126  accuracy:0.96913\n",
      "[Train] loss=0.22786  accuracy:0.96336\n",
      "[Train] loss=0.20423  accuracy:0.95290\n",
      "[Train] loss=0.20209  accuracy:0.95870\n",
      "[Test] loss=0.30039  accuracy:0.90897\n",
      "echos: 3\n",
      "[Train] loss=0.19017  accuracy:0.98433\n",
      "[Train] loss=0.19703  accuracy:0.98231\n",
      "[Train] loss=0.20158  accuracy:0.98360\n",
      "[Train] loss=0.17556  accuracy:0.98648\n",
      "[Train] loss=0.17534  accuracy:0.98035\n",
      "[Train] loss=0.16936  accuracy:0.97902\n",
      "[Train] loss=0.14740  accuracy:0.97348\n",
      "[Train] loss=0.15398  accuracy:0.97492\n",
      "[Test] loss=0.27059  accuracy:0.90793\n",
      "echos: 4\n",
      "[Train] loss=0.15050  accuracy:0.98796\n",
      "[Train] loss=0.15531  accuracy:0.98843\n",
      "[Train] loss=0.16321  accuracy:0.98647\n",
      "[Train] loss=0.14440  accuracy:0.98777\n",
      "[Train] loss=0.13548  accuracy:0.98727\n",
      "[Train] loss=0.13781  accuracy:0.98215\n",
      "[Train] loss=0.11806  accuracy:0.97984\n",
      "[Train] loss=0.12305  accuracy:0.98322\n",
      "[Test] loss=0.25514  accuracy:0.91060\n",
      "echos: 5\n",
      "[Train] loss=0.12635  accuracy:0.99042\n",
      "[Train] loss=0.12809  accuracy:0.98989\n",
      "[Train] loss=0.13583  accuracy:0.98923\n",
      "[Train] loss=0.12070  accuracy:0.98812\n",
      "[Train] loss=0.11358  accuracy:0.98948\n",
      "[Train] loss=0.11556  accuracy:0.98700\n",
      "[Train] loss=0.09765  accuracy:0.98791\n",
      "[Train] loss=0.10643  accuracy:0.98642\n",
      "[Test] loss=0.24889  accuracy:0.90838\n",
      "echos: 6\n",
      "[Train] loss=0.10690  accuracy:0.99404\n",
      "[Train] loss=0.11120  accuracy:0.99242\n",
      "[Train] loss=0.11767  accuracy:0.99131\n",
      "[Train] loss=0.10141  accuracy:0.99184\n",
      "[Train] loss=0.10242  accuracy:0.98920\n",
      "[Train] loss=0.09958  accuracy:0.98951\n",
      "[Train] loss=0.08552  accuracy:0.98748\n",
      "[Train] loss=0.09614  accuracy:0.98586\n",
      "[Test] loss=0.25320  accuracy:0.90021\n",
      "echos: 7\n",
      "[Train] loss=0.09425  accuracy:0.99495\n",
      "[Train] loss=0.10067  accuracy:0.99348\n",
      "[Train] loss=0.10244  accuracy:0.99299\n",
      "[Train] loss=0.09209  accuracy:0.99289\n",
      "[Train] loss=0.08982  accuracy:0.99142\n",
      "[Train] loss=0.09419  accuracy:0.98747\n",
      "[Train] loss=0.07625  accuracy:0.98727\n",
      "[Train] loss=0.08376  accuracy:0.98887\n",
      "[Test] loss=0.26524  accuracy:0.88759\n",
      "echos: 8\n",
      "[Train] loss=0.08726  accuracy:0.99482\n",
      "[Train] loss=0.08955  accuracy:0.99481\n",
      "[Train] loss=0.09487  accuracy:0.99477\n",
      "[Train] loss=0.07955  accuracy:0.99604\n",
      "[Train] loss=0.07964  accuracy:0.99308\n",
      "[Train] loss=0.08060  accuracy:0.99076\n",
      "[Train] loss=0.06798  accuracy:0.98769\n",
      "[Train] loss=0.07334  accuracy:0.98906\n",
      "[Test] loss=0.29631  accuracy:0.86472\n",
      "echos: 9\n",
      "[Train] loss=0.08346  accuracy:0.99132\n",
      "[Train] loss=0.08214  accuracy:0.99348\n",
      "[Train] loss=0.08756  accuracy:0.99407\n",
      "[Train] loss=0.07324  accuracy:0.99616\n",
      "[Train] loss=0.07254  accuracy:0.99460\n",
      "[Train] loss=0.08614  accuracy:0.98716\n",
      "[Train] loss=0.06000  accuracy:0.99066\n",
      "[Train] loss=0.06821  accuracy:0.98982\n",
      "[Test] loss=0.29690  accuracy:0.86130\n",
      "[  1. 128.   1. ...  30.  21.  14.]\n"
     ]
    }
   ],
   "source": [
    "echos = 10\n",
    "batch_size = 500\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(echos):\n",
    "        print('echos: %d' % i);\n",
    "        start_index = 0\n",
    "        while (start_index+batch_size < collectionMatrix.shape[0]*0.9):\n",
    "            batch_x0 = usersMatrix[start_index: start_index+batch_size]\n",
    "            batch_x1 = collectionMatrix[start_index: start_index+batch_size]\n",
    "            batch_y = collectionMatrix[start_index: start_index+batch_size]\n",
    "            index = [i for i in range(batch_size)]\n",
    "            random.shuffle(index)\n",
    "            batch_x0 = batch_x0[index]\n",
    "            batch_x1 = batch_x1[index]\n",
    "            batch_y = batch_y[index]\n",
    "            loss_val, acc_val, _ = sess.run(\n",
    "                [loss, accuracy, train_op], feed_dict = {x0:batch_x0, x1:batch_x1, y:batch_y, dropout_rate:0.5})\n",
    "            print ('[Train] loss=%4.5f  accuracy:%4.5f'% (loss_val,acc_val))\n",
    "            start_index += batch_size\n",
    "        test_x0 = usersMatrix[start_index:]\n",
    "        test_x1 = collectionMatrix[start_index:]\n",
    "        test_y = collectionMatrix[start_index:] \n",
    "        test_loss, test_acc = sess.run(\n",
    "            [loss, accuracy], feed_dict = {x0:test_x0, x1:test_x1, y:test_y, dropout_rate:1.0})\n",
    "        print('[Test] loss=%4.5f  accuracy:%4.5f'% (test_loss, test_acc))\n",
    "    y_values, predict_resource = sess.run(\n",
    "        [y_out_prob, predict], feed_dict = {x0: usersMatrix, x1:collectionMatrix, y:collectionMatrix, dropout_rate:1.0})\n",
    "    print(predict_resource.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   1,    1,    1, ..., 4641, 4641, 4641], dtype=int64), array([   9,   13,   34, ...,  657,  790, 1222], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "recommend_resource = np.subtract(predict_resource, collectionMatrix)\n",
    "recommend_resource[recommend_resource < 0] = 0\n",
    "recommend_resource_indices = np.nonzero(recommend_resource)\n",
    "print(recommend_resource_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345481,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_resource_indices[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
